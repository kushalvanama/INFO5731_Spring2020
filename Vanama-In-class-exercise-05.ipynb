{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Vanama-In-class-exercise-05.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-UbFktH6DBN"
      },
      "source": [
        "## The fifth In-class-exercise (2/23/2021, 20 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHoMKeXK6DBQ"
      },
      "source": [
        "In exercise-03, I asked you to collected 500 textual data based on your own information needs (If you didn't collect the textual data, you should recollect for this exercise). Now we need to think about how to represent the textual data for text classification. In this exercise, you are required to select 10 types of features (10 types of features but absolutely more than 10 features) in the followings feature list, then represent the 500 texts with these features. The output should be in the following format:\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "The feature list:\n",
        "\n",
        "* (1) tf-idf features\n",
        "* (2) POS-tag features: number of adjective, adverb, auxiliary, punctuation, complementizer, coordinating conjunction, subordinating conjunction, determiner, interjection, noun, possessor, preposition, pronoun, quantifier, verb, and other. (select some of them if you use pos-tag features)\n",
        "* (3) Linguistic features:\n",
        "  * number of right-branching nodes across all constituent types\n",
        "  * number of right-branching nodes for NPs only\n",
        "  * number of left-branching nodes across all constituent types\n",
        "  * number of left-branching nodes for NPs only\n",
        "  * number of premodifiers across all constituent types\n",
        "  * number of premodifiers within NPs only\n",
        "  * number of postmodifiers across all constituent types\n",
        "  * number of postmodifiers within NPs only\n",
        "  * branching index across all constituent types, i.e. the number of right-branching nodes minus number of left-branching nodes\n",
        "  * branching index for NPs only\n",
        "  * branching weight index: number of tokens covered by right-branching nodes minus number of tokens covered by left-branching nodes across all categories\n",
        "  * branching weight index for NPs only \n",
        "  * modification index, i.e. the number of premodifiers minus the number of postmodifiers across all categories\n",
        "  * modification index for NPs only\n",
        "  * modification weight index: length in tokens of all premodifiers minus length in tokens of all postmodifiers across all categories\n",
        "  * modification weight index for NPs only\n",
        "  * coordination balance, i.e. the maximal length difference in coordinated constituents\n",
        "  \n",
        "  * density (density can be calculated using the ratio of folowing function words to content words) of determiners/quantifiers\n",
        "  * density of pronouns\n",
        "  * density of prepositions\n",
        "  * density of punctuation marks, specifically commas and semicolons\n",
        "  * density of auxiliary verbs\n",
        "  * density of conjunctions\n",
        "  * density of different pronoun types: Wh, 1st, 2nd, and 3rd person pronouns\n",
        "  \n",
        "  * maximal and average NP length\n",
        "  * maximal and average AJP length\n",
        "  * maximal and average PP length\n",
        "  * maximal and average AVP length\n",
        "  * sentence length\n",
        "\n",
        "* Other features in your mind (ie., pre-defined patterns)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buUoqHwo6DBR",
        "outputId": "c327f3fb-4046-4ae0-fb9f-7ada6814b9db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# You code here (Please add comments in the code):\r\n",
        "\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "import json\r\n",
        "\r\n",
        "def extractData(data): # Extracts title and contents of article from html data\r\n",
        "  soup = BeautifulSoup(data, 'html.parser')\r\n",
        "  headline_list = []\r\n",
        "  body_list = []\r\n",
        "  all_news = soup.findAll(\"div\",{\"class\":\"news-card z-depth-1\"})\r\n",
        "  count = len(all_news)\r\n",
        "  for each_news in all_news:\r\n",
        "    article_body = each_news.find(\"div\", {\"itemprop\" : \"articleBody\"}).get_text()\r\n",
        "    headline = each_news.find(\"span\", {\"itemprop\" : \"headline\"}).get_text()\r\n",
        "    headline_list.append(headline)\r\n",
        "    body_list.append(article_body)\r\n",
        "  \r\n",
        "  return headline_list, body_list\r\n",
        "\r\n",
        "def get_headers(): # The headers for the http request\r\n",
        "    return {\r\n",
        "        \"accept\": \"*/*\",\r\n",
        "        \"accept-encoding\": \"gzip, deflate, br\",\r\n",
        "        \"accept-language\": \"en-IN,en-US;q=0.9,en;q=0.8\",\r\n",
        "        \"content-type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\r\n",
        "        \"cookie\": \"_ga=GA1.2.474379061.1548476083; _gid=GA1.2.251903072.1548476083; __gads=ID=17fd29a6d34048fc:T=1548476085:S=ALNI_MaRiLYBFlMfKNMAtiW0J3b_o0XGxw\",\r\n",
        "        \"origin\": \"https://inshorts.com\",\r\n",
        "        \"referer\": \"https://inshorts.com/en/read/\",\r\n",
        "        \"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\",\r\n",
        "        \"x-requested-with\": \"XMLHttpRequest\"\r\n",
        "    }\r\n",
        "\r\n",
        "title_text = [] # List to store News Titles\r\n",
        "content_text =[] #List to store News Content\r\n",
        "\r\n",
        "temp_title = []\r\n",
        "temp_content = []\r\n",
        "\r\n",
        "url = \"https://www.inshorts.com/en/read\"\r\n",
        "request = requests.get(url)\r\n",
        "title_text, content_text = extractData(request.text)  # Extract articles in First page - 25 articles\r\n",
        "\r\n",
        "regex_min_news_id = re.compile('var min_news_id = \"(.*?)\";')\r\n",
        "min_news_id = regex_min_news_id.search(request.text).group(1)\r\n",
        "\r\n",
        "for number in range(475): # Extract 475 more articles\r\n",
        "  ajax_url = 'https://inshorts.com/en/ajax/more_news'\r\n",
        "  response = requests.post(ajax_url, data={\"category\": \"\", \"news_offset\": min_news_id}, headers=get_headers())\r\n",
        "  try:\r\n",
        "    response_json = json.loads(response.text)\r\n",
        "  except:\r\n",
        "    break\r\n",
        "  temp_title, temp_content = extractData(response_json[\"html\"])\r\n",
        "  title_text.append(temp_title)\r\n",
        "  content_text.append(temp_content)\r\n",
        "  temp_title = []\r\n",
        "  temp_content = []\r\n",
        "  min_news_id = response_json[\"min_news_id\"]\r\n",
        "\r\n",
        "df = pd.DataFrame(content_text, columns =['Article Content'])  # Creating Dataframe\r\n",
        "df"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article Content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A lifelong promise provides care, trust and se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Congress on Saturday announced its first list ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The BJP has announced its first list of 57 can...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The Congress has released its first list of 40...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>India reported 18,711 fresh cases of COVID-19 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>[At least 11 people were killed and 36 others ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>[A puppy named 'Cyclops', who was born with on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>[A lake has formed above Raini village in Utta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>[In her reply to the Budget discussions in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>[Ahead of Assembly elections in the state, the...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>203 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       Article Content\n",
              "0    A lifelong promise provides care, trust and se...\n",
              "1    Congress on Saturday announced its first list ...\n",
              "2    The BJP has announced its first list of 57 can...\n",
              "3    The Congress has released its first list of 40...\n",
              "4    India reported 18,711 fresh cases of COVID-19 ...\n",
              "..                                                 ...\n",
              "198  [At least 11 people were killed and 36 others ...\n",
              "199  [A puppy named 'Cyclops', who was born with on...\n",
              "200  [A lake has formed above Raini village in Utta...\n",
              "201  [In her reply to the Budget discussions in the...\n",
              "202  [Ahead of Assembly elections in the state, the...\n",
              "\n",
              "[203 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60CXC7zrJo2I",
        "outputId": "8b428353-cbc8-4d5b-e946-2dbfdfeb8984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "for index, row in df.iterrows():\r\n",
        "    if isinstance(row['Article Content'], list):\r\n",
        "      row['Article Content'] = \"\".join(row['Article Content'])\r\n",
        "\r\n",
        "df"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article Content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A lifelong promise provides care, trust and se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Congress on Saturday announced its first list ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The BJP has announced its first list of 57 can...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The Congress has released its first list of 40...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>India reported 18,711 fresh cases of COVID-19 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>At least 11 people were killed and 36 others i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>A puppy named 'Cyclops', who was born with one...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>A lake has formed above Raini village in Uttar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>In her reply to the Budget discussions in the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>Ahead of Assembly elections in the state, the ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>203 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       Article Content\n",
              "0    A lifelong promise provides care, trust and se...\n",
              "1    Congress on Saturday announced its first list ...\n",
              "2    The BJP has announced its first list of 57 can...\n",
              "3    The Congress has released its first list of 40...\n",
              "4    India reported 18,711 fresh cases of COVID-19 ...\n",
              "..                                                 ...\n",
              "198  At least 11 people were killed and 36 others i...\n",
              "199  A puppy named 'Cyclops', who was born with one...\n",
              "200  A lake has formed above Raini village in Uttar...\n",
              "201  In her reply to the Budget discussions in the ...\n",
              "202  Ahead of Assembly elections in the state, the ...\n",
              "\n",
              "[203 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K430_gdM8C_9",
        "outputId": "1acb8395-0ca1-4124-8b25-748fee4e8a97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#data cleaning and pre processing\r\n",
        "import string\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "words=stopwords.words('english')\r\n",
        "from textblob import Word\r\n",
        "from textblob import TextBlob\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('wordnet')\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "st=PorterStemmer()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2LfxXn88afh",
        "outputId": "0ede9f93-c533-4104-8b0d-adc8bcd192ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# To lower_case\r\n",
        "df['Article Content']=df['Article Content'].apply(lambda x:\" \".join(x.lower() for x in x.split()))\r\n",
        "\r\n",
        "# remove punctuation\r\n",
        "df['Article Content']=df['Article Content'].apply(lambda  x: \" \".join(x for x in x.split() if x not in string.punctuation))\r\n",
        "\r\n",
        "# remove special characters\r\n",
        "df['Article Content']=df['Article Content'].apply(lambda x:\" \".join(x.replace('[#,@,&,!,$,^,*]', '') for x in x.split()))\r\n",
        "\r\n",
        "# remove stop words\r\n",
        "df['Article Content']=df['Article Content'].apply(lambda x:\" \".join(x for x in x.split() if x not in words))\r\n",
        "\r\n",
        "# remove numbers\r\n",
        "df['Article Content']=df['Article Content'].apply(lambda x:\" \".join(x.replace('\\d+', '') for x in x.split()))\r\n",
        "\r\n",
        "# tokenize\r\n",
        "df['Article Content']=df['Article Content'].apply(lambda x: TextBlob(x).words)\r\n",
        "\r\n",
        "# stemming\r\n",
        "df['Article Content']=df['Article Content'].apply(lambda x: \" \".join([st.stem(word) for word in x]))\r\n",
        "\r\n",
        "# lemmatization\r\n",
        "df['Article Content']=df['Article Content'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\r\n",
        "\r\n",
        "df"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article Content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lifelong promis provid care trust secur say ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>congress saturday announc first list 13 candid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bjp announc first list 57 candid first two pha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>congress releas first list 40 candid upcom thr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>india report 18,711 fresh case covid-19 100 de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>least 11 peopl kill 36 other injur fire firecr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>puppi name 'cyclop born one eye two tongu nose...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>lake form raini villag uttarakhand 's chamoli ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>repli budget discus parliament financ minist n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>ahead assembl elect state assam govern friday ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>203 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       Article Content\n",
              "0    lifelong promis provid care trust secur say ca...\n",
              "1    congress saturday announc first list 13 candid...\n",
              "2    bjp announc first list 57 candid first two pha...\n",
              "3    congress releas first list 40 candid upcom thr...\n",
              "4    india report 18,711 fresh case covid-19 100 de...\n",
              "..                                                 ...\n",
              "198  least 11 peopl kill 36 other injur fire firecr...\n",
              "199  puppi name 'cyclop born one eye two tongu nose...\n",
              "200  lake form raini villag uttarakhand 's chamoli ...\n",
              "201  repli budget discus parliament financ minist n...\n",
              "202  ahead assembl elect state assam govern friday ...\n",
              "\n",
              "[203 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhpSVotw_G-m"
      },
      "source": [
        "**TF-IDF Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grflkMvN_MD9"
      },
      "source": [
        "def term_frequency(string):\r\n",
        "    from collections import Counter\r\n",
        "    tf = {}\r\n",
        "    counts = Counter(string.split())\r\n",
        "    for key, value in counts.items():\r\n",
        "        tf[key] = round((value/len(counts.keys())), 3)\r\n",
        "    return tf"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARU0v9GZ_SX1",
        "outputId": "f7069cc1-04b7-427c-a90b-ba685068e71d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df['tf'] = df['Article Content'].apply(term_frequency)\r\n",
        "df['tf']"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      {'lifelong': 0.062, 'promis': 0.062, 'provid':...\n",
              "1      {'congress': 0.059, 'saturday': 0.029, 'announ...\n",
              "2      {'bjp': 0.061, 'announc': 0.03, 'first': 0.061...\n",
              "3      {'congress': 0.059, 'releas': 0.029, 'first': ...\n",
              "4      {'india': 0.091, 'report': 0.03, '18,711': 0.0...\n",
              "                             ...                        \n",
              "198    {'least': 0.001, '11': 0.001, 'peopl': 0.007, ...\n",
              "199    {'puppi': 0.004, 'name': 0.004, ''cyclop': 0.0...\n",
              "200    {'lake': 0.009, 'form': 0.002, 'raini': 0.002,...\n",
              "201    {'repli': 0.001, 'budget': 0.003, 'discus': 0....\n",
              "202    {'ahead': 0.001, 'assembl': 0.001, 'elect': 0....\n",
              "Name: tf, Length: 203, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LZvIthb_hSu"
      },
      "source": [
        "from math import log\r\n",
        "def inverse_document_fequency(array):\r\n",
        "  word_counts_by_row = []\r\n",
        "  for row in array:\r\n",
        "      d = dict.fromkeys(row.split(' '), 0)\r\n",
        "      word_counts_by_row.append(d)\r\n",
        "  \r\n",
        "  for d in word_counts_by_row:\r\n",
        "      for word in d.keys():\r\n",
        "          for row in array:\r\n",
        "              if word in row.split(' '):\r\n",
        "                  v = d[word]\r\n",
        "                  v += 1\r\n",
        "                  d[word] = v\r\n",
        "  for d in word_counts_by_row:\r\n",
        "      for key, value in d.items():\r\n",
        "          d[key] = log(array.shape[0]/value)\r\n",
        "  return word_counts_by_row"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-9PPPKZ_o0u",
        "outputId": "0b1edc58-1d7e-4613-babc-0f99461395be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "df['idf'] = inverse_document_fequency(df['Article Content'].values)\r\n",
        "df['idf']"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-fdcb01009d05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'idf'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minverse_document_fequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Article Content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'idf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-6b796c2ed66a>\u001b[0m in \u001b[0;36minverse_document_fequency\u001b[0;34m(array)\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m               \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                   \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                   \u001b[0mv\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP_WtmNC_3y_"
      },
      "source": [
        "def tf_idf(array1, array2):\r\n",
        "    tf_idf_by_row = []\r\n",
        "    for d1, d2 in zip(array1, array2):\r\n",
        "        d = {}\r\n",
        "        for key in d1.keys():\r\n",
        "            d[key] = d1[key]*d2[key]\r\n",
        "        tf_idf_by_row.append(d)\r\n",
        "    return tf_idf_by_row"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dpCOtMx_6gz",
        "outputId": "761eec44-57b5-44d1-a7f7-39e8819269dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "df['tf_idf'] = tf_idf(df['tf'].values, df['idf'].values)\r\n",
        "df['tf_idf']"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'idf'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-6081e9752f27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tf_idf'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'idf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tf_idf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'idf'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG3zzl5mAAbA"
      },
      "source": [
        "**POS Tag Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnMhzS30-Ctw"
      },
      "source": [
        "# Tagging and Counting POS\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "from nltk.tag import pos_tag\r\n",
        "from collections import Counter\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "df['Tokens']=df['Article Content'].apply(lambda x: TextBlob(x).words)\r\n",
        "\r\n",
        "pos=[]\r\n",
        "for i in df['Tokens']:\r\n",
        "  pos.append(nltk.pos_tag(i))\r\n",
        "pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxl07JExA55A"
      },
      "source": [
        "**Linguistic Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5B2IvYhA-Rq"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "def calculate_features(tag_pos):\r\n",
        "\r\n",
        "  counts = np.zeros(10)\r\n",
        "  nn=0\r\n",
        "  dt=0\r\n",
        "  verb=0\r\n",
        "  cord_conj=0\r\n",
        "  pro=0\r\n",
        "  pos_end=0\r\n",
        "  adv=0\r\n",
        "  adj=0\r\n",
        "  prep=0\r\n",
        "  pre_det=0\r\n",
        "\r\n",
        "  nouns=[]\r\n",
        "  det=[]\r\n",
        "  verbs=[]\r\n",
        "  coordinate_conjuction=[]\r\n",
        "  pronoun=[]\r\n",
        "  posend=[]\r\n",
        "  adverb=[]\r\n",
        "  adjective=[]\r\n",
        "  preposition=[]\r\n",
        "  predet=[]\r\n",
        "  for i in tag_pos:\r\n",
        "    for j in i:\r\n",
        "      if(j[1]=='NN'):\r\n",
        "        counts[0]+=1\r\n",
        "      elif (j[1]=='DT'):\r\n",
        "        counts[1]+=1\r\n",
        "      elif (j[1]=='VB'):\r\n",
        "        counts[2]+=1\r\n",
        "      elif (j[1]=='CC'):\r\n",
        "        counts[3]+=1\r\n",
        "      elif (j[1]=='PRP'):\r\n",
        "        counts[4]+=1\r\n",
        "      elif (j[1]=='POS'):\r\n",
        "        counts[5]+=1\r\n",
        "      elif (j[1]=='RB'):\r\n",
        "        counts[6]+=1\r\n",
        "      elif (j[1]=='JJ'):\r\n",
        "        counts[7]+=1\r\n",
        "      elif (j[1]=='IN'):\r\n",
        "        counts[8]+=1\r\n",
        "      elif (j[1]=='PDT'):\r\n",
        "        counts[9]+=1\r\n",
        "    nouns.append(nn)\r\n",
        "    det.append(dt)\r\n",
        "    verbs.append(verb)\r\n",
        "    coordinate_conjuction.append(cord_conj)\r\n",
        "    pronoun.append(pro)\r\n",
        "    posend.append(pos_end)\r\n",
        "    adverb.append(adv)\r\n",
        "    adjective.append(adj)\r\n",
        "    preposition.append(prep)\r\n",
        "    predet.append(pre_det)\r\n",
        "    counts.fill(0) \r\n",
        "\r\n",
        "  return nouns,det,verbs,coordinate_conjuction,pronoun,posend,adverb,adjective,preposition,predet\r\n",
        "\r\n",
        "count_pos=calculate_features(pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGpOC59WC3eU"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "zippedlist=list(zip(count_pos[0],count_pos[1],count_pos[2],count_pos[3],count_pos[4],count_pos[5],count_pos[6],count_pos[7],count_pos[8],count_pos[9]))\r\n",
        "df_total = pd.DataFrame(zippedlist, columns=['Nouns','Determiner','Verbs','Coordinating conjunction','Personal pronoun','Possessive ending','Adverb','Adjective','Preposition','Predeterminer'])\r\n",
        "df_total.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJIF-7N0DKVU"
      },
      "source": [
        "df_tokens=df['Tokens']\r\n",
        "df_tokens.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3raK12GmDQiB"
      },
      "source": [
        "final_df = pd.concat([df_tokens, df_total],ignore_index=True, sort=False,axis=1)\r\n",
        "final_df.columns=['text','Nouns','Determiner','Verbs','Coordinating conjunction','Personal pronoun','Possessive ending','Adverb','Adjective','Preposition','Predeterminer']\r\n",
        "final_df.head(13)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}