{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO5731_Assignment_Two.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [2019 Dell labtop](https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/dp/B07N49F51N/ref=sr_1_11?crid=1IJ7UWF2F4GHH&keywords=dell%2Bxps%2B15&qid=1580173569&sprefix=dell%2Caps%2C181&sr=8-11&th=1) on amazon.\n",
        "\n",
        "(2) Collect the top 100 User Reviews of the film [Joker](https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv) from IMDB.\n",
        "\n",
        "(3) Collect the abstracts of the top 100 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(4) Collect the top 100 tweets by using hashtag [\"#wuhancoronovirus\"](https://twitter.com/hashtag/wuhancoronovirus) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01d1ca90-ee30-44f5-c598-957ef9771626"
      },
      "source": [
        "# Write your code here\n",
        "#Install necessary packages\n",
        "\n",
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\r\u001b[K     |▍                               | 10kB 15.6MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 21.4MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 15.4MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40kB 16.4MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51kB 14.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 61kB 15.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 71kB 14.3MB/s eta 0:00:01\r\u001b[K     |███                             | 81kB 13.3MB/s eta 0:00:01\r\u001b[K     |███▎                            | 92kB 12.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 102kB 12.1MB/s eta 0:00:01\r\u001b[K     |████                            | 112kB 12.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 122kB 12.1MB/s eta 0:00:01\r\u001b[K     |████▊                           | 133kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 143kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 153kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 163kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 174kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 184kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 194kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 204kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 215kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 225kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 235kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 245kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 256kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 266kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 276kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 286kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 296kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 307kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 317kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 327kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 337kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 348kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 358kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 368kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 378kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 389kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 399kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 409kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 419kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 430kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 440kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 450kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 460kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 471kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 481kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 491kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 501kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 512kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 522kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 532kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 542kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 552kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 563kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 573kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 583kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 593kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 604kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 614kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 624kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 634kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 645kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 655kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 665kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 675kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 686kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 696kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 706kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 716kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 727kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 737kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 747kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 757kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 768kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 778kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 788kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 798kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 808kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 819kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 829kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 839kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 849kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 860kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 870kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 880kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 890kB 12.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 901kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 911kB 12.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:6 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Ign:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,394 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,163 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,392 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,963 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,738 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [889 kB]\n",
            "Fetched 10.8 MB in 3s (4,225 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 32 not upgraded.\n",
            "Need to get 81.0 MB of archives.\n",
            "After this operation, 273 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 87.0.4280.66-0ubuntu0.18.04.1 [1,122 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 87.0.4280.66-0ubuntu0.18.04.1 [71.7 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 87.0.4280.66-0ubuntu0.18.04.1 [3,716 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 87.0.4280.66-0ubuntu0.18.04.1 [4,488 kB]\n",
            "Fetched 81.0 MB in 2s (53.7 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 149406 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_87.0.4280.66-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "VUztpFHmz6pi",
        "outputId": "21201ba3-320a-4ed8-ceb1-07f64ccbcba1"
      },
      "source": [
        "# Collection of top 100 reviews of Joker and creation of a dataframe\r\n",
        "from selenium import webdriver\r\n",
        "from selenium.webdriver.support.wait import WebDriverWait\r\n",
        "import time\r\n",
        "from time import sleep\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "chrome_options = webdriver.ChromeOptions()\r\n",
        "chrome_options.add_argument('--headless')\r\n",
        "chrome_options.add_argument('--no-sandbox')\r\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\r\n",
        "driver =webdriver.Chrome('chromedriver',options=chrome_options)\r\n",
        "url='https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv'\r\n",
        "wait=WebDriverWait(driver,10)\r\n",
        "driver.get(url)\r\n",
        "\r\n",
        "for i in range(3):\r\n",
        "  button=driver.find_element_by_class_name('ipl-load-more__button')\r\n",
        "  time.sleep(2)\r\n",
        "  button.click()\r\n",
        "  time.sleep(5)\r\n",
        "time.sleep(10)\r\n",
        "data=driver.page_source\r\n",
        "driver.quit()\r\n",
        "soup=BeautifulSoup(data,'html.parser')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "names=[]\r\n",
        "for div in soup.find_all(name='div',attrs={\"class\":\"display-name-date\"}):\r\n",
        "  for span in div.find_all(name='span',attrs={\"class\":\"display-name-link\"}):\r\n",
        "    names.append(span.text)\r\n",
        "\r\n",
        "\r\n",
        "reviews=[]\r\n",
        "for div in soup.find_all(name='div',attrs={'class':'content'}):\r\n",
        "  for di in div.find_all(name='div',attrs={'class':'show-more__control'}):\r\n",
        "    reviews.append(di.text)\r\n",
        "\r\n",
        "#zippedList = list(zip(a,b))\r\n",
        "df = pd.DataFrame({'User Name':names, 'Review': reviews})\r\n",
        "#df = pd.DataFrame(zippedList, columns=['User Name','Review'])\r\n",
        "df\r\n",
        "       "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User Name</th>\n",
              "      <th>Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MihaVrhunc</td>\n",
              "      <td>Every once in a while a movie comes, that trul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lesterarnoldpinto</td>\n",
              "      <td>This is a movie that only those who have felt ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Aman_Goyal</td>\n",
              "      <td>Truly a masterpiece, The Best Hollywood film o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>logical_guy</td>\n",
              "      <td>Joaquin Phoenix gives a tour de force performa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>kdagoulis26</td>\n",
              "      <td>Most of the time movies are anticipated like t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>The CyberHippie</td>\n",
              "      <td>This film tries desperately to devillify the m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>wongcalvin</td>\n",
              "      <td>Wow I honestly gotta tell you, it's one of the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>kernelmilkshake-67766</td>\n",
              "      <td>The whole point of this movie is to bring atte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>danteshamest</td>\n",
              "      <td>While I enjoyed the film, it felt pretty short...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>indigomontoya</td>\n",
              "      <td>There is doing an homage and then there is bor...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                User Name                                             Review\n",
              "0              MihaVrhunc  Every once in a while a movie comes, that trul...\n",
              "1       lesterarnoldpinto  This is a movie that only those who have felt ...\n",
              "2              Aman_Goyal  Truly a masterpiece, The Best Hollywood film o...\n",
              "3             logical_guy  Joaquin Phoenix gives a tour de force performa...\n",
              "4             kdagoulis26  Most of the time movies are anticipated like t...\n",
              "..                    ...                                                ...\n",
              "95        The CyberHippie  This film tries desperately to devillify the m...\n",
              "96             wongcalvin  Wow I honestly gotta tell you, it's one of the...\n",
              "97  kernelmilkshake-67766  The whole point of this movie is to bring atte...\n",
              "98           danteshamest  While I enjoyed the film, it felt pretty short...\n",
              "99          indigomontoya  There is doing an homage and then there is bor...\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4QalAaQ2PqM"
      },
      "source": [
        "# Copy to CSV file\r\n",
        "pd.DataFrame(df).to_csv('joker_reviews.csv',header=True,index=None)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "097bdf2d-2d42-49f5-f8c0-292d704ad338"
      },
      "source": [
        "# Write your code here\n",
        "# Import and download required packages\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "words=stopwords.words('english')\n",
        "from textblob import Word\n",
        "from textblob import TextBlob\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import PorterStemmer\n",
        "st=PorterStemmer()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkhYvK1t3F2l",
        "outputId": "a9c91cd5-6cce-4d40-a5b4-c6dccf6d5c46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# Clean the text data\r\n",
        "# To lower_case\r\n",
        "df['Clean_Review']=df['Review'].apply(lambda x:\" \".join(x.lower() for x in x.split()))\r\n",
        "\r\n",
        "# remove punctuation\r\n",
        "df['Clean_Review']=df['Clean_Review'].apply(lambda  x: \" \".join(x for x in x.split() if x not in string.punctuation))\r\n",
        "\r\n",
        "# remove special characters\r\n",
        "df['Clean_Review']=df['Clean_Review'].apply(lambda x:\" \".join(x.replace('[#,@,&,!,$,^,*]', '') for x in x.split()))\r\n",
        "\r\n",
        "# remove stop words\r\n",
        "df['Clean_Review']=df['Clean_Review'].apply(lambda x:\" \".join(x for x in x.split() if x not in words))\r\n",
        "\r\n",
        "# remove numbers\r\n",
        "df['Clean_Review']=df['Clean_Review'].apply(lambda x:\" \".join(x.replace('\\d+', '') for x in x.split()))\r\n",
        "\r\n",
        "# correct spellings\r\n",
        "df['Clean_Review']=df['Clean_Review'].apply(lambda x: str(TextBlob(x).correct()))\r\n",
        "\r\n",
        "# tokenize\r\n",
        "df['Clean_Review']=df['Clean_Review'].apply(lambda x: TextBlob(x).words)\r\n",
        "\r\n",
        "# stemming\r\n",
        "df['Clean_Review']=df['Clean_Review'].apply(lambda x: \" \".join([st.stem(word) for word in x]))\r\n",
        "\r\n",
        "# lemmatization\r\n",
        "df['Clean_Review']=df['Clean_Review'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\r\n",
        "\r\n",
        "df_update = df.drop('Review', axis=1).copy()\r\n",
        "\r\n",
        "df_update\r\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User Name</th>\n",
              "      <th>Clean_Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MihaVrhunc</td>\n",
              "      <td>everi movi come truli make impact joaquin 's p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lesterarnoldpinto</td>\n",
              "      <td>movi felt alon isol truli relat it understand ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Aman_Goyal</td>\n",
              "      <td>truli masterpiec best hollywood film 2019 one ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>logical_guy</td>\n",
              "      <td>joaquin phoenix give tour de forc perform fear...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>kdagoulis26</td>\n",
              "      <td>time move anticip like end fall short way shor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>The CyberHippie</td>\n",
              "      <td>film tri desper devillifi stylish charismat vi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>wongcalvin</td>\n",
              "      <td>now honestli gutta tell you one best move i 'v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>kernelmilkshake-67766</td>\n",
              "      <td>whole point movi bring attent mental ill i 'll...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>danteshamest</td>\n",
              "      <td>enjoy film felt pretti short the end appear sc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>indigomontoya</td>\n",
              "      <td>homag borrow wholesal materi film littl make t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                User Name                                       Clean_Review\n",
              "0              MihaVrhunc  everi movi come truli make impact joaquin 's p...\n",
              "1       lesterarnoldpinto  movi felt alon isol truli relat it understand ...\n",
              "2              Aman_Goyal  truli masterpiec best hollywood film 2019 one ...\n",
              "3             logical_guy  joaquin phoenix give tour de forc perform fear...\n",
              "4             kdagoulis26  time move anticip like end fall short way shor...\n",
              "..                    ...                                                ...\n",
              "95        The CyberHippie  film tri desper devillifi stylish charismat vi...\n",
              "96             wongcalvin  now honestli gutta tell you one best move i 'v...\n",
              "97  kernelmilkshake-67766  whole point movi bring attent mental ill i 'll...\n",
              "98           danteshamest  enjoy film felt pretti short the end appear sc...\n",
              "99          indigomontoya  homag borrow wholesal materi film littl make t...\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwTeXhY38KPx",
        "outputId": "35c6fd70-bd99-4b7b-8db8-b9d2a94d62de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Import and download require packages\r\n",
        "import spacy\r\n",
        "from spacy import displacy\r\n",
        "nlp=spacy.load('en_core_web_sm')\r\n",
        "!pip install benepar\r\n",
        "%tensorflow_version 1.x\r\n",
        "import benepar\r\n",
        "benepar.download('benepar_en2')\r\n",
        "from benepar.spacy_plugin import BeneparComponent"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: benepar in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from benepar) (3.12.4)\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.7/dist-packages (from benepar) (3.2.5)\n",
            "Requirement already satisfied: spacy>=2.0.9 in /usr/local/lib/python3.7/dist-packages (from benepar) (2.2.4)\n",
            "Requirement already satisfied: torch-struct>=0.5 in /usr/local/lib/python3.7/dist-packages (from benepar) (0.5)\n",
            "Requirement already satisfied: transformers[tokenizers,torch]>=4.2.2 in /usr/local/lib/python3.7/dist-packages (from benepar) (4.3.3)\n",
            "Requirement already satisfied: tokenizers>=0.9.4 in /usr/local/lib/python3.7/dist-packages (from benepar) (0.10.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from benepar) (1.7.1+cu101)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.7/dist-packages (from benepar) (0.1.95)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->benepar) (53.0.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->benepar) (1.15.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (3.0.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (3.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (0.0.43)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->benepar) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers[tokenizers,torch]>=4.2.2->benepar) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers[tokenizers,torch]>=4.2.2->benepar) (3.4.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[tokenizers,torch]>=4.2.2->benepar) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[tokenizers,torch]>=4.2.2->benepar) (7.1.2)\n",
            "[nltk_data] Error loading benepar_en2: Package 'benepar_en2' not found\n",
            "[nltk_data]     in index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3c494317-1d49-4598-ba5f-0bbf8738e19e"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "# 3-1 Tagging and Counting POS\n",
        "from collections import Counter\n",
        "def tag_counts(review):\n",
        "    tokens = nlp(review)\n",
        "    pos = [token.pos_ for token in tokens]\n",
        "    counts = Counter(pos)\n",
        "    return counts\n",
        "\n",
        "df_update['Tags'] = df_update['Clean_Review'].apply(tag_counts)\n",
        "df_update.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User Name</th>\n",
              "      <th>Clean_Review</th>\n",
              "      <th>Tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MihaVrhunc</td>\n",
              "      <td>everi movi come truli make impact joaquin 's p...</td>\n",
              "      <td>{'PROPN': 11, 'VERB': 9, 'NOUN': 16, 'PART': 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lesterarnoldpinto</td>\n",
              "      <td>movi felt alon isol truli relat it understand ...</td>\n",
              "      <td>{'PROPN': 14, 'VERB': 11, 'PRON': 1, 'NOUN': 8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Aman_Goyal</td>\n",
              "      <td>truli masterpiec best hollywood film 2019 one ...</td>\n",
              "      <td>{'PROPN': 18, 'ADV': 4, 'NOUN': 27, 'NUM': 2, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>logical_guy</td>\n",
              "      <td>joaquin phoenix give tour de forc perform fear...</td>\n",
              "      <td>{'PROPN': 23, 'VERB': 9, 'ADJ': 6, 'NOUN': 11,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>kdagoulis26</td>\n",
              "      <td>time move anticip like end fall short way shor...</td>\n",
              "      <td>{'NOUN': 11, 'VERB': 6, 'ADV': 5, 'INTJ': 1, '...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           User Name  ...                                               Tags\n",
              "0         MihaVrhunc  ...  {'PROPN': 11, 'VERB': 9, 'NOUN': 16, 'PART': 1...\n",
              "1  lesterarnoldpinto  ...  {'PROPN': 14, 'VERB': 11, 'PRON': 1, 'NOUN': 8...\n",
              "2         Aman_Goyal  ...  {'PROPN': 18, 'ADV': 4, 'NOUN': 27, 'NUM': 2, ...\n",
              "3        logical_guy  ...  {'PROPN': 23, 'VERB': 9, 'ADJ': 6, 'NOUN': 11,...\n",
              "4        kdagoulis26  ...  {'NOUN': 11, 'VERB': 6, 'ADV': 5, 'INTJ': 1, '...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dNJTudi8D9t"
      },
      "source": [
        "#3-2\r\n",
        "#Dependency Parsing\r\n",
        "for i in df_update['Clean_Review']:\r\n",
        "  token=nlp(i)\r\n",
        "  text= list(token.sents)\r\n",
        "  displacy.render(text, style='dep', jupyter=True, options={'distance': 90})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JmgegCd9A8U"
      },
      "source": [
        "# Constituency Parsing\r\n",
        "\r\n",
        "# Loading spaCy’s en model and adding benepar model to its pipeline\r\n",
        "nlp = spacy.load('en')\r\n",
        "nlp.add_pipe(BeneparComponent('benepar_en2'))\r\n",
        "\r\n",
        "\r\n",
        "# Generating a parse tree for the text\r\n",
        "for i in df_update['Clean_Review']:\r\n",
        "  print(list(nlp(i).sents)[0]._.parse_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AhYy6wm9eHD"
      },
      "source": [
        "# Extract entiries and calculate the count\r\n",
        "\r\n",
        "text=[]\r\n",
        "label=[]\r\n",
        "for i in df_update['lemmatization']:\r\n",
        "  j=nlp(i)\r\n",
        "  for ent in j.ents:\r\n",
        "    text.append(ent.text)\r\n",
        "    label.append(ent.label_)\r\n",
        "    print(\"Text :\",ent.text,\" \",\"Label: \",ent.label_)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iSbEOVz9yuC"
      },
      "source": [
        "from collections import Counter\r\n",
        "mylabels=set(label)\r\n",
        "mylabels\r\n",
        "occurrences = Counter(label)\r\n",
        "occurrences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIEH3QBu97EA"
      },
      "source": [
        "'''\r\n",
        "Dependency Parsing Tree:\r\n",
        "\r\n",
        "Dependency parsing tree is the process of analysing the grammatical structure of a sentence based on the dependencies between the words \r\n",
        "in a sentence.\r\n",
        "\r\n",
        "Example:\r\n",
        "consider the sentence 'rainy weather' . In this sentence, the word rainy modifies the meaning of noun weather. Therefore dependency exists\r\n",
        "from weather -> rainy.\r\n",
        "\r\n",
        "Constituency Parsing Tree:\r\n",
        "\r\n",
        "It is the process of analyzing the sentences by breaking down it into sub phrases. These sub phrases may belong to a category of grammar \r\n",
        "like NP(noun phrase) and VP(verb phrase).\r\n",
        "\r\n",
        "Example:\r\n",
        "\r\n",
        "(S (NP (DT every) (NN movie)) (VP (VP (VBZ comes)) (, ,) (VP (ADVP (RB truly)) (VB make) (NP (NN impact))))\r\n",
        "\r\n",
        "The above one is the parse tree in the form of a string.\r\n",
        "\r\n",
        "The S, NP, VP, ADVP, NP.... represents the constituents.\r\n",
        "DT, NN, VBZ, RB, VB -> pos tags\r\n",
        "every, movie,  comes, truly, make, impact -> words of sentences.\r\n",
        "\r\n",
        "\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}