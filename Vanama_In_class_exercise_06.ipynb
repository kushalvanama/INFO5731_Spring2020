{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "In_class_exercise_05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kushalvanama/INFO5731_Spring2020/blob/main/Vanama_In_class_exercise_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7TahL04sVvR"
      },
      "source": [
        "# **The sixth in-class-exercise (20 points in total, 3/2/2021)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejyZITr8sjnh"
      },
      "source": [
        "## **1. Rule-based information extraction (10 points)**\n",
        "\n",
        "Use any keywords related to data science, natural language processing, machine learning to search from google scholar, get the **titles** of 100 articles (either by web scraping or manually) about this topic, define a set of patterns to extract the research questions/problems, methods/algorithms/models, datasets, applications, or any other important information about this topic. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "XvR_O9D8sOUY",
        "outputId": "a0f5535c-6f6f-4a2e-ed17-7c1e73dd2782"
      },
      "source": [
        "# Write your code here\r\n",
        "\r\n",
        "from IPython.core.display import display, HTML\r\n",
        "display(HTML('<style>.container { width:80% !important; }</style>'))\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import requests\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "headers = [{'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:81.0) Gecko/20100101 Firefox/81.0'},\r\n",
        "           {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36 Vivaldi/3.3'},\r\n",
        "           {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'},\r\n",
        "           {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36 Edg/86.0.622.38'}]\r\n",
        "\r\n",
        "pages = np.arange(1, 11) * 10 \r\n",
        "\r\n",
        "responses = []\r\n",
        "for page in pages:\r\n",
        "    url = 'https://scholar.google.com/scholar?start=' + str(page) + '&q=natural+language+processing&hl=en&as_sdt=0,44'\r\n",
        "    print(f'{page} titles being extracted')\r\n",
        "    random_index = np.random.randint(0, 4)\r\n",
        "    header = headers[random_index]\r\n",
        "    response = requests.get(url, headers=header)\r\n",
        "    if response.status_code == 200:\r\n",
        "        print('Everything is fine. Continue to scrap ...')\r\n",
        "    else:\r\n",
        "        print('Something went wrong!')\r\n",
        "    soup=BeautifulSoup(response.content,'lxml')\r\n",
        "    responses.append(soup)\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.container { width:80% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "10 titles being extracted\n",
            "Everything is fine. Continue to scrap ...\n",
            "20 titles being extracted\n",
            "Everything is fine. Continue to scrap ...\n",
            "30 titles being extracted\n",
            "Everything is fine. Continue to scrap ...\n",
            "40 titles being extracted\n",
            "Everything is fine. Continue to scrap ...\n",
            "50 titles being extracted\n",
            "Everything is fine. Continue to scrap ...\n",
            "60 titles being extracted\n",
            "Everything is fine. Continue to scrap ...\n",
            "70 titles being extracted\n",
            "Everything is fine. Continue to scrap ...\n",
            "80 titles being extracted\n",
            "Everything is fine. Continue to scrap ...\n",
            "90 titles being extracted\n",
            "Everything is fine. Continue to scrap ...\n",
            "100 titles being extracted\n",
            "Everything is fine. Continue to scrap ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsaAy4JUOHk-"
      },
      "source": [
        "titles = []\r\n",
        "count = 1\r\n",
        "for response in responses:\r\n",
        "    for item in response.select('[data-lid]'):\r\n",
        "        try:\r\n",
        "            titles.append(item.select('h3')[0].get_text())\r\n",
        "        except Exception as error:\r\n",
        "            print('Something went wrong!')\r\n",
        "        count += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PerbCCEWOOVf",
        "outputId": "1a7a6623-1bb9-4339-a968-73ca316c415a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.DataFrame({'titles' : titles})\r\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>titles</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Natural language processing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[BOOK][B] Natural language processing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[PDF][PDF] Natural language processing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[BOOK][B] Strategies for natural language proc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[PDF][PDF] Human engineering fcr applied natur...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              titles\n",
              "0                        Natural language processing\n",
              "1              [BOOK][B] Natural language processing\n",
              "2             [PDF][PDF] Natural language processing\n",
              "3  [BOOK][B] Strategies for natural language proc...\n",
              "4  [PDF][PDF] Human engineering fcr applied natur..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkvHonjWORPW"
      },
      "source": [
        "df.to_csv('titles.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUiOSljHOZIK"
      },
      "source": [
        "**DATA PRE-PROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IevQAV9BOdUw",
        "outputId": "17ce77b6-0f31-490c-b523-f0196ea53e9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import spacy\r\n",
        "nlp = spacy.load('en_core_web_sm')\r\n",
        "pd.set_option('display.max_colwidth', 200)\r\n",
        "\r\n",
        "df = pd.read_csv('titles.csv')\r\n",
        "df.titles.values"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Natural language processing',\n",
              "       '[BOOK][B] Natural language processing',\n",
              "       '[PDF][PDF] Natural language processing',\n",
              "       '[BOOK][B] Strategies for natural language processing',\n",
              "       '[PDF][PDF] Human engineering fcr applied natural language processing',\n",
              "       'Practical Natural-Language Processing by Computer',\n",
              "       'A primer on neural network models for natural language processing',\n",
              "       '[BOOK][B] Introduction to natural language processing',\n",
              "       'Allennlp: A deep semantic natural language processing platform',\n",
              "       'Introduction to Arabic natural language processing',\n",
              "       'A maximum entropy approach to natural language processing',\n",
              "       '[PDF][PDF] Natural language processing',\n",
              "       '[BOOK][B] The handbook of computational linguistics and natural language processing',\n",
              "       \"HuggingFace's Transformers: State-of-the-art natural language processing\",\n",
              "       '[PDF][PDF] Natural language processing (almost) from scratch',\n",
              "       'An overview of empirical natural language processing',\n",
              "       '[PDF][PDF] Natural language processing in medicine: an overview',\n",
              "       'Natural language processing: a historical review',\n",
              "       '[BOOK][B] Natural language processing and text mining',\n",
              "       'Natural language processing for information retrieval',\n",
              "       '[BOOK][B] Deep learning in natural language processing',\n",
              "       'Neural network methods for natural language processing',\n",
              "       'Automated encoding of clinical documents based on natural language processing',\n",
              "       '[PDF][PDF] Natural language processing and its future in medicine',\n",
              "       'Arabic natural language processing: Challenges and solutions',\n",
              "       'Unlocking clinical data from narrative reports: a study of natural language processing',\n",
              "       'Ask me anything: Dynamic memory networks for natural language processing',\n",
              "       'Recent trends in deep learning based natural language processing',\n",
              "       'Comparative study of CNN and RNN for natural language processing',\n",
              "       'A broad-coverage natural language processing system.',\n",
              "       'Natural language processing and information retrieval',\n",
              "       '[PDF][PDF] GENIES: a natural-language processing system for the extraction of molecular pathways from journal articles',\n",
              "       'Natural language processing: State of the art, current trends and challenges',\n",
              "       'Sentiment analysis: Capturing favorability using natural language processing',\n",
              "       '[HTML][HTML] What can natural language processing do for clinical decision support?',\n",
              "       'Natural language processing and the representation of clinical data',\n",
              "       '[PDF][PDF] Fudannlp: A toolkit for chinese natural language processing',\n",
              "       'A unified architecture for natural language processing: Deep neural networks with multitask learning',\n",
              "       '[BOOK][B] Natural language processing: the PLNLP approach',\n",
              "       '[BOOK][B] Biomedical natural language processing',\n",
              "       'Web-based models for natural language processing',\n",
              "       'Natural language processing',\n",
              "       'Jumping NLP curves: A review of natural language processing research',\n",
              "       'Genomics and natural language processing',\n",
              "       'KoNLPy: Korean natural language processing in Python',\n",
              "       'Natural language processing and language learning',\n",
              "       'A survey of the usages of deep learning for natural language processing',\n",
              "       'Scispacy: Fast and robust models for biomedical natural language processing',\n",
              "       'Natural language processing in oncology: a review',\n",
              "       '[HTML][HTML] A pattern dictionary for natural language processing',\n",
              "       'Using natural-language processing to produce weather forecasts',\n",
              "       '[PDF][PDF] A simple introduction to maximum entropy models for natural language processing',\n",
              "       'A literature survey of active machine learning in the context of natural language processing',\n",
              "       'Deep learning for natural language processing: advantages and challenges',\n",
              "       '[PDF][PDF] GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural Language Processing.',\n",
              "       'Transformers: State-of-the-art natural language processing',\n",
              "       '[HTML][HTML] A linguistic ontology of space for natural language processing',\n",
              "       'Introduction to Chinese natural language processing',\n",
              "       '[BOOK][B] Evaluating natural language processing systems: An analysis and review',\n",
              "       '[PDF][PDF] Transfer learning in natural language processing',\n",
              "       '[BOOK][B] Python natural language processing',\n",
              "       'Detection of duplicate defect reports using natural language processing',\n",
              "       'Natural language processing in radiology: a systematic review',\n",
              "       '[PDF][PDF] Review on natural language processing',\n",
              "       'Chengqing Zong: Statistical natural language processing',\n",
              "       'Natural language processing and semantical representation of medical texts',\n",
              "       'Natural language processing of lyrics',\n",
              "       '[BOOK][B] Challenges in natural language processing',\n",
              "       'Natural language processing for social media',\n",
              "       'Lexical knowledge representation and natural language processing',\n",
              "       'Analyzing discourse processing using a simple natural language processing tool',\n",
              "       '[BOOK][B] Practical structured learning techniques for natural language processing',\n",
              "       '[BOOK][B] Natural language processing for Prolog programmers',\n",
              "       'Natural language processing future',\n",
              "       '[PDF][PDF] A survey on hate speech detection using natural language processing',\n",
              "       '[BOOK][B] Spotting and discovering terms through natural language processing',\n",
              "       '[BOOK][B] Natural language processing with Java',\n",
              "       '[PDF][PDF] Natural language processing and information retrieval',\n",
              "       'NL-OOPS: from natural language to object oriented requirements using the natural language processing system LOLITA',\n",
              "       'Wikipedia-based semantic interpretation for natural language processing',\n",
              "       'Stanza: A Python natural language processing toolkit for many human languages',\n",
              "       '[PDF][PDF] Neural transfer learning for natural language processing',\n",
              "       'Pre-trained models for natural language processing: A survey',\n",
              "       'Data statements for natural language processing: Toward mitigating system bias and enabling better science',\n",
              "       '[PDF][PDF] The social impact of natural language processing',\n",
              "       'Processing natural language without natural language processing',\n",
              "       'Mitigating gender bias in natural language processing: Literature review',\n",
              "       '[PDF][PDF] A general organization of knowledge for natural language processing: the penman upper model',\n",
              "       'The Paninian approach to natural language processing',\n",
              "       'Natural language processing in an operational clinical information system',\n",
              "       'Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging',\n",
              "       'NLP (natural language processing) for NLP (natural language programming)',\n",
              "       'Natural language processing for enhancing teaching and learning',\n",
              "       \"[PDF][PDF] The hitchhiker's guide to testing statistical significance in natural language processing\",\n",
              "       '[PDF][PDF] Natural Language Processing in Information Retrieval.',\n",
              "       'Similarity-based approaches to natural language processing',\n",
              "       'Opportunities for natural language processing research in education',\n",
              "       'Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques',\n",
              "       'Prediction and substantiation: A new approach to natural language processing',\n",
              "       '[HTML][HTML] Natural language processing to extract medical problems from electronic clinical documents: performance evaluation'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MDvwoYtOkZn",
        "outputId": "bb88533e-2be8-4605-e4fd-495fe9eaa45f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def clean_title(s):\r\n",
        "    discards = ['[PDF][PDF] ', '[BOOK][B] ', '[HTML][HTML] ', '[CITATION][C] ']\r\n",
        "    for discard in discards:\r\n",
        "        if discard in s:\r\n",
        "            s = s.replace(discard, '')\r\n",
        "    return s\r\n",
        "df.titles = df.titles.apply(clean_title)\r\n",
        "df.titles"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                                                            Natural language processing\n",
              "1                                                                                            Natural language processing\n",
              "2                                                                                            Natural language processing\n",
              "3                                                                             Strategies for natural language processing\n",
              "4                                                              Human engineering fcr applied natural language processing\n",
              "                                                             ...                                                        \n",
              "95                                                            Similarity-based approaches to natural language processing\n",
              "96                                                   Opportunities for natural language processing research in education\n",
              "97            Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques\n",
              "98                                          Prediction and substantiation: A new approach to natural language processing\n",
              "99    Natural language processing to extract medical problems from electronic clinical documents: performance evaluation\n",
              "Name: titles, Length: 100, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FENPhwDnOoiU",
        "outputId": "656d47a6-50d4-4dde-ed56-a7829bbd3d2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def clean_text(text):\r\n",
        "    import re\r\n",
        "    # removing apostrophes\r\n",
        "    text = re.sub(\"'s\",'',str(text))\r\n",
        "    # removing hyphens\r\n",
        "    text = re.sub(\"-\",' ',str(text))\r\n",
        "    text = re.sub(\"— \",'',str(text))\r\n",
        "    # removing quotation marks\r\n",
        "    text = re.sub('\\\"','',str(text))\r\n",
        "    # removing any reference to outside text\r\n",
        "    text = re.sub('[^\\w\\s]','', str(text))\r\n",
        "    return text\r\n",
        "\r\n",
        "df.titles = df.titles.apply(clean_text)\r\n",
        "df.titles"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                                                           Natural language processing\n",
              "1                                                                                           Natural language processing\n",
              "2                                                                                           Natural language processing\n",
              "3                                                                            Strategies for natural language processing\n",
              "4                                                             Human engineering fcr applied natural language processing\n",
              "                                                            ...                                                        \n",
              "95                                                           Similarity based approaches to natural language processing\n",
              "96                                                  Opportunities for natural language processing research in education\n",
              "97            Sentiment analyzer Extracting sentiments about a given topic using natural language processing techniques\n",
              "98                                          Prediction and substantiation A new approach to natural language processing\n",
              "99    Natural language processing to extract medical problems from electronic clinical documents performance evaluation\n",
              "Name: titles, Length: 100, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk1zTWbHOxpq",
        "outputId": "55750fde-4967-4633-ea9f-d008dd3a9fd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def words(text):\r\n",
        "    from spacy.matcher import Matcher\r\n",
        "    word_ = []\r\n",
        "    doc = nlp(text)\r\n",
        "    patterns = [[{'LOWER':'new'},\r\n",
        "                 {'POS':'NOUN'}],\r\n",
        "              [{'LOWER':'lexical'},\r\n",
        "              {'POS':'NOUN'}],\r\n",
        "              [{'LOWER':'pattern'},\r\n",
        "              {'POS':'NOUN'}],\r\n",
        "              [{'LOWER':'system'},\r\n",
        "              {'POS':'NOUN'}],\r\n",
        "              [{'LOWER':'using'},\r\n",
        "              {'POS':'NOUN'}]]\r\n",
        "    matcher = Matcher(nlp.vocab)\r\n",
        "    for pattern in patterns:\r\n",
        "        matcher.add(\"matching\", None, pattern)\r\n",
        "        matches = matcher(doc)\r\n",
        "        for i in range(0,len(matches)):\r\n",
        "            token = doc[matches[i][1]:matches[i][2]]\r\n",
        "            word_.append(str(token))\r\n",
        "        for word in word_:\r\n",
        "            print(word)\r\n",
        "    return word_\r\n",
        "\r\n",
        "df.titles.apply(words).values"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lexical knowledge\n",
            "Lexical knowledge\n",
            "Lexical knowledge\n",
            "Lexical knowledge\n",
            "Lexical knowledge\n",
            "Lexical knowledge\n",
            "Lexical knowledge\n",
            "Lexical knowledge\n",
            "Lexical knowledge\n",
            "Lexical knowledge\n",
            "system bias\n",
            "system bias\n",
            "system bias\n",
            "new approach\n",
            "new approach\n",
            "new approach\n",
            "new approach\n",
            "new approach\n",
            "new approach\n",
            "new approach\n",
            "new approach\n",
            "new approach\n",
            "new approach\n",
            "new approach\n",
            "new approach\n",
            "new approach\n",
            "new approach\n",
            "new approach\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list([]), list([]),\n",
              "       list(['Lexical knowledge', 'Lexical knowledge', 'Lexical knowledge', 'Lexical knowledge']),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list(['system bias', 'system bias']), list([]), list([]),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list(['new approach', 'new approach', 'new approach', 'new approach', 'new approach']),\n",
              "       list([])], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBB90fO1O3d8",
        "outputId": "84ea498f-9e3c-4d3c-dca5-8f547b09eac0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def pattern(text):\r\n",
        "    from spacy.matcher import Matcher\r\n",
        "    patterns_ = []\r\n",
        "    doc = nlp(text)\r\n",
        "    prog_list = ['method', 'approach','analysis',\r\n",
        "                 'techniques','research']\r\n",
        "    patterns = [[{'LOWER':{'IN':prog_list},'OP':'+'}],\r\n",
        "               [{'POS': 'NOUN'}, {'LOWER': 'of'}, {'POS':'NOUN'}],\r\n",
        "               [{'POS': 'VERB'}, {'LOWER': 'for'}, {'POS':'NOUN'}]]\r\n",
        "    matcher = Matcher(nlp.vocab) \r\n",
        "    for pattern in patterns:\r\n",
        "        matcher.add(\"matching\", None, pattern) \r\n",
        "        matches = matcher(doc)\r\n",
        "        for i in range(0,len(matches)):\r\n",
        "            start, end = matches[i][1], matches[i][2]\r\n",
        "            if doc[start].pos_=='DET':\r\n",
        "                start = start+1\r\n",
        "            span = str(doc[start:end])\r\n",
        "            if (len(patterns_)!=0) and (patterns_[-1] in span):\r\n",
        "                patterns_[-1] = span\r\n",
        "            else:\r\n",
        "                patterns_.append(span)\r\n",
        "    return patterns_\r\n",
        "\r\n",
        "df.titles.apply(pattern).values"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list([]), list([]), list([]), list(['approach']),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list([]), list([]), list([]), list(['analysis']),\n",
              "       list([]), list([]), list([]), list([]), list(['approach']),\n",
              "       list([]), list([]), list([]), list(['research']), list([]),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list(['ontology of space']), list([]), list(['analysis']),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list(['processing of lyrics']), list([]), list([]),\n",
              "       list([]), list([]), list(['techniques']), list([]), list([]),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list([]), list([]), list([]), list([]), list([]),\n",
              "       list([]), list(['organization of knowledge']), list(['approach']),\n",
              "       list([]), list(['part of speech']), list([]), list([]), list([]),\n",
              "       list([]), list([]), list(['research']), list(['techniques']),\n",
              "       list(['approach']), list([])], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq_7VGmrsum4"
      },
      "source": [
        "## **2. Domain-specific information extraction (10 points)**\n",
        "\n",
        "For the legal case used in the data cleaning exercise: [01-05-1 Adams v Tanner.txt](https://raw.githubusercontent.com/unt-iialab/info5731_spring2021/main/class_exercises/01-05-1%20%20Adams%20v%20Tanner.txt), use [legalNLP](https://lexpredict-lexnlp.readthedocs.io/en/latest/modules/extract/extract.html#nlp-based-extraction-methods) to extract the following inforation from the text (if the information is not exist, just print None):\n",
        "\n",
        "(1) acts, e.g., “section 1 of the Advancing Hope Act, 1986”\n",
        "\n",
        "(2) amounts, e.g., “ten pounds” or “5.8 megawatts”\n",
        "\n",
        "(3) citations, e.g., “10 U.S. 100” or “1998 S. Ct. 1”\n",
        "\n",
        "(4) companies, e.g., “Lexpredict LLC”\n",
        "\n",
        "(5) conditions, e.g., “subject to …” or “unless and until …”\n",
        "\n",
        "(6) constraints, e.g., “no more than”\n",
        "\n",
        "(7) copyright, e.g., “(C) Copyright 2000 Acme”\n",
        "\n",
        "(8) courts, e.g., “Supreme Court of New York”\n",
        "\n",
        "(9) CUSIP, e.g., “392690QT3”\n",
        "\n",
        "(10) dates, e.g., “June 1, 2017” or “2018-01-01”\n",
        "\n",
        "(11) definitions, e.g., “Term shall mean …”\n",
        "\n",
        "(12) distances, e.g., “fifteen miles”\n",
        "\n",
        "(13) durations, e.g., “ten years” or “thirty days”\n",
        "\n",
        "(14) geographic and geopolitical entities, e.g., “New York” or “Norway”\n",
        "\n",
        "(15) money and currency usages, e.g., “$5” or “10 Euro”\n",
        "\n",
        "(16) percents and rates, e.g., “10%” or “50 bps”\n",
        "\n",
        "(17) PII, e.g., “212-212-2121” or “999-999-9999”\n",
        "\n",
        "(18) ratios, e.g.,” 3:1” or “four to three”\n",
        "\n",
        "(19) regulations, e.g., “32 CFR 170”\n",
        "\n",
        "(20) trademarks, e.g., “MyApp (TM)”\n",
        "\n",
        "(21) URLs, e.g., “http://acme.com/”\n",
        "\n",
        "(22) addresses, e.g., “1999 Mount Read Blvd, Rochester, NY, USA, 14615”\n",
        "\n",
        "(23) persons, e.g., “John Doe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEUGZ7JtRf8u",
        "outputId": "4e962338-d4cc-4d40-f81e-a92956db555e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install lexnlp==0.2.7\r\n",
        "import nltk\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lexnlp==0.2.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/50/e5e769dfa27b9c657bc3fefb6edee56d186c630176746e232030aa5409ed/lexnlp-0.2.7-py3-none-any.whl (8.0MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0MB 6.4MB/s \n",
            "\u001b[?25hCollecting gensim==3.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/a6/82ee7b14c204b82ec00e91fc6b67331cc7b28460ad72b2214384abd0e0a3/gensim-3.4.0.tar.gz (22.2MB)\n",
            "\u001b[K     |████████████████████████████████| 22.2MB 1.9MB/s \n",
            "\u001b[?25hCollecting pandas==0.23.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/a7/12261a51ac2e7be4c698ca27cbe364ca5f16d64999456ee47ea8c7b44417/pandas-0.23.4-cp37-cp37m-manylinux1_x86_64.whl (8.8MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8MB 23.1MB/s \n",
            "\u001b[?25hCollecting pycountry==18.5.26\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/c0/8ce9d2b55347867900edbe4d18f790571130c16f882b4891a0f08627dcdc/pycountry-18.5.26-py2.py3-none-any.whl (10.3MB)\n",
            "\u001b[K     |████████████████████████████████| 10.3MB 27.5MB/s \n",
            "\u001b[?25hCollecting regex==2017.9.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/65/91b43adad1dc45d7374521422270490128a2f289e1c3e1036b231b521507/regex-2017.09.23.tar.gz (607kB)\n",
            "\u001b[K     |████████████████████████████████| 614kB 46.4MB/s \n",
            "\u001b[?25hCollecting us==1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/72/83/8731cbf5afcf3434c0b24cfc520c11fd27bfc8a6878114662f4e3dbdab71/us-1.0.0.tar.gz\n",
            "Collecting num2words==0.5.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/d8/1c1fb47cce56ff2cc1f5eb2740f2679045769778a746fbf9ebff1d70a63e/num2words-0.5.5-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.0MB/s \n",
            "\u001b[?25hCollecting scipy==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/73/76fc6ea21818eed0de8dd38e1e9586725578864169a2b31acdeffb9131c8/scipy-1.0.0.tar.gz (15.2MB)\n",
            "\u001b[K     |████████████████████████████████| 15.2MB 190kB/s \n",
            "\u001b[?25hCollecting typing==3.6.2\n",
            "  Downloading https://files.pythonhosted.org/packages/44/88/d09c6a7fe1af4a02f16d2f1766212bec752aadb04e5699a9706a10a1a37d/typing-3.6.2-py3-none-any.whl\n",
            "Collecting scikit-learn==0.19.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/2c/5edf2488897cad4fb8c4ace86369833552615bf264460ae4ef6e1f258982/scikit-learn-0.19.1.tar.gz (9.5MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5MB 30.4MB/s \n",
            "\u001b[?25hCollecting datefinder-lexpredict==0.6.2\n",
            "  Downloading https://files.pythonhosted.org/packages/27/47/9a38724045b30e2e4d1c5e3e08fd3b0770dedb2e9ca92c1347b9e2182470/datefinder_lexpredict-0.6.2-py2.py3-none-any.whl\n",
            "Collecting reporters-db==1.0.12.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/6c/16c7c3849a25d2c3af5ef6e05d768d8e86e74aa9051df4728325c5f31f46/reporters_db-1.0.12.1-py2.py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.1MB/s \n",
            "\u001b[?25hCollecting dateparser==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/9e/1aa87c0c59f9731820bfd20a8b148d97b315530c2c92d1fb300328c8c42f/dateparser-0.7.0-py2.py3-none-any.whl (357kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 41.3MB/s \n",
            "\u001b[?25hCollecting nltk==3.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/c2/858e0708b497116ae45cf5c6b1f66984ac60729c61e49df6c1c0b808d1e4/nltk-3.2.4.tar.gz (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 43.0MB/s \n",
            "\u001b[?25hCollecting requests==2.22.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.0MB/s \n",
            "\u001b[?25hCollecting Unidecode==0.4.21\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/a1/9d7f3138ee3d79a1ab865a2cb38200ca778d85121db19fe264c76c981184/Unidecode-0.04.21-py2.py3-none-any.whl (228kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 55.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==3.4.0->lexnlp==0.2.7) (1.19.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.4.0->lexnlp==0.2.7) (1.15.0)\n",
            "Requirement already satisfied: smart_open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.4.0->lexnlp==0.2.7) (4.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from pandas==0.23.4->lexnlp==0.2.7) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.7/dist-packages (from pandas==0.23.4->lexnlp==0.2.7) (2018.9)\n",
            "Collecting jellyfish==0.5.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/48/ddb1458d966f0a84e472d059d87a9d1527df7768a725132fc1d810728386/jellyfish-0.5.6.tar.gz (132kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 56.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser==0.7.0->lexnlp==0.2.7) (1.5.1)\n",
            "Collecting idna<2.9,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->lexnlp==0.2.7) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->lexnlp==0.2.7) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->lexnlp==0.2.7) (1.24.3)\n",
            "Building wheels for collected packages: gensim, regex, us, scipy, scikit-learn, nltk, jellyfish\n",
            "  Building wheel for gensim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gensim: filename=gensim-3.4.0-cp37-cp37m-linux_x86_64.whl size=23316475 sha256=e2084b626c8837be750f5f0f94a4157a790a64856da569babc64f71d74233cec\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/5f/2d/04fe5cffea90fbba14c8eab40f519096c8558cceaaa6777048\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.9.23-cp37-cp37m-linux_x86_64.whl size=539783 sha256=59adbe1c75c9ebdb9673b8e9f381b3c84fe8fcee82cdb0f7c022a897a8884bf6\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/19/41/e7d239b4a53386fe9de49f9e4328799569bbeac8b8b3748876\n",
            "  Building wheel for us (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for us: filename=us-1.0.0-cp37-none-any.whl size=11833 sha256=14410ece641c6d9cc3dacb7a4155188c274bee65b093d15849bab51865da3b3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/98/40/cb8be35d7779a0ae4372c84e7a585c947bfc41540fd8999e53\n",
            "  Building wheel for scipy (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for scipy\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for scipy\n",
            "\u001b[31m  ERROR: Failed cleaning build dir for scipy\u001b[0m\n",
            "  Building wheel for scikit-learn (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for scikit-learn\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for scikit-learn\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.2.4-cp37-none-any.whl size=1367705 sha256=5b725a68028b7f8e713d2a0c5c8e180ea3694de4cfc6649783967cc1bec738cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/f1/5c/f667347d86a3a534ba4c0127eed4389f929916e3ec88bb461a\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.5.6-cp37-cp37m-linux_x86_64.whl size=71989 sha256=798205aa5129e2dbfccedb21fdc3e5ab311b174f5579a6b23e0e0c371c11cc65\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/29/06/8d686d24f742cb89e7bde7f26f18cb9e89b3c8bcd6999cb12a\n",
            "Successfully built gensim regex us nltk jellyfish\n",
            "Failed to build scipy scikit-learn\n",
            "\u001b[31mERROR: yellowbrick 0.9.1 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement pandas>=0.25, but you'll have pandas 0.23.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.5.1 has requirement scikit-learn>=0.22, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.23.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.23.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imbalanced-learn 0.4.3 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.23.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.23.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement scipy>=1.1.0, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy, gensim, pandas, pycountry, regex, jellyfish, us, num2words, typing, scikit-learn, datefinder-lexpredict, reporters-db, dateparser, nltk, idna, requests, Unidecode, lexnlp\n",
            "  Found existing installation: scipy 1.5.1\n",
            "    Uninstalling scipy-1.5.1:\n",
            "      Successfully uninstalled scipy-1.5.1\n",
            "    Running setup.py install for scipy ... \u001b[?25l\u001b[?25herror\n",
            "  Rolling back uninstall of scipy\n",
            "  Moving to /usr/local/lib/python3.7/dist-packages/scipy-1.5.1.dist-info/\n",
            "   from /usr/local/lib/python3.7/dist-packages/~cipy-1.5.1.dist-info\n",
            "  Moving to /usr/local/lib/python3.7/dist-packages/scipy.libs/\n",
            "   from /usr/local/lib/python3.7/dist-packages/~cipy.libs\n",
            "  Moving to /usr/local/lib/python3.7/dist-packages/scipy/\n",
            "   from /usr/local/lib/python3.7/dist-packages/~cipy\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-0jhcz5kd/scipy/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-0jhcz5kd/scipy/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-wjvogtr4/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc7NtJrLx5tS",
        "outputId": "86971183-dfbc-4eb2-c907-d0799506be3f"
      },
      "source": [
        "# write your code here\r\n",
        "import lexnlp\r\n",
        "import re\r\n",
        "import lexnlp.extract.en.acts\r\n",
        "\r\n",
        "with open('01-05-1 Adams v Tanner.txt', 'r', encoding=\"utf8\") as file:\r\n",
        "    text = file.read()\r\n",
        "text = re.sub(r'\\n',' ', str(text))\r\n",
        "#acts\r\n",
        "lexnlp.extract.en.acts.get_act_list(text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHwr0ZKcPQzy",
        "outputId": "1a6da5c3-80bc-4c01-e00f-a8f043bccb7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# amounts\r\n",
        "import lexnlp.extract.en.amounts\r\n",
        "for amount in list(lexnlp.extract.en.amounts.get_amounts(text)):\r\n",
        "    print(amount, end=', ')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.0, 740.0, 1843.0, 2.0, 1.0, 4.0, 2.0, 1821.0, 5.0, 1.0, 1840.0, 3777.0, 80.0, 100.0, 30.0, 1839.0, 741.0, 22.0, 1840.0, 14000.0, 120.0, 1.0, 1840.0, 3.0, 4.0, 1.0, 1.0, 1840.0, 2.0, 1.0, 361.0, 1.0, 307.0, 6.0, 604.0, 1.0, 2.0, 418.0, 422.0, 7.0, 34.0, 41.0, 167.0, 742.0, 3.0, 112.0, 207.0, 3.0, 338.0, 424.0, 5.0, 26.0, 13.0, 235.0, 8.0, 693.0, 4.0, 1821.0, 167.0, 2.0, 2.0, 216.0, 3.0, 66.0, 4.0, 130.0, 29.0, 2.0, 241.0, 2.0, 332.0, 2.0, 422.0, 9.0, 112.0, 743.0, 9.0, 39.0, 14000.0, 1840.0, 744.0, 5.0, 182.0, 3.0, 368.0, 1.0, 397.0, 6.0, 604.0, 1.0, 1821.0, 167.0, 745.0, 4.0, 746.0, 4.0, 210.0, 46.0, 747.0, 5.0, 5.0, 740.0, 1843.0, 284.0, 2019.0, 9.0, 1.0, 55.0, 266.0, 271.0, 1876.0, 2.0, 47.0, 362.0, 376.0, 1872.0, 3.0, 45.0, 329.0, 334.0, 1871.0, 4.0, 31.0, 526.0, 527.0, 1858.0, 5.0, 21.0, 333.0, 335.0, 1852.0, 6.0, 8.0, 145.0, 147.0, 1857.0, 7.0, 65.0, 256.0, 258.0, 3.0, 1880.0, 8.0, 4.0, 913.0, 914.0, 1887.0, 9.0, 103.0, 464.0, 1936.0, 3.0, 1.0, 9.0, 39.0, 1828.0, 2.0, 2.0, 5.0, 182.0, 1837.0, 2.0, 3.0, 9.0, 108.0, 1812.0, 6.0, 1.0, 2.0, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVlVhLOWPRan",
        "outputId": "760cf56c-d561-4fe7-d3c0-076d571c194a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import lexnlp.extract.en.citations\r\n",
        "_ = [print(citation) for citation in list(lexnlp.extract.en.citations.get_citations(text))]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 'Ala.', 'Alabama Reports', 740, None, None, None)\n",
            "(5, 'Ala.', 'Alabama Reports', 740, '1843', None, None)\n",
            "(55, 'Ala.', 'Alabama Reports', 266, '271', None, None)\n",
            "(47, 'Ala.', 'Alabama Reports', 362, '376', None, None)\n",
            "(45, 'Ala.', 'Alabama Reports', 329, '334', None, None)\n",
            "(31, 'Ala.', 'Alabama Reports', 526, '527', None, None)\n",
            "(21, 'Ala.', 'Alabama Reports', 333, '335', None, None)\n",
            "(8, 'Cal.', 'California Reports', 145, '147', None, None)\n",
            "(65, 'Ala.', 'Alabama Reports', 256, '258', None, None)\n",
            "(4, 'S.W.', 'South Western Reporter', 913, '914', None, None)\n",
            "(103, 'A.L.R.', 'American Law Reports', 464, None, None, None)\n",
            "(9, 'Cow.', \"Cowen's Reports\", 39, None, None, None)\n",
            "(5, 'Port.', 'Alabama Reports, Porter', 182, None, None, None)\n",
            "(9, 'Johns.', \"Johnson's Reports\", 108, None, None, None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX8k1jOFPVO2",
        "outputId": "bb12b6fb-cf37-4a40-8fd7-7035fc7d017e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# companies\r\n",
        "import lexnlp.extract.en.entities.nltk_re\r\n",
        "_ = [print(company) for company in list(lexnlp.extract.en.entities.nltk_re.get_companies(text))]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lehman, Durr Co, (17983, 18001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w335f72pPXjJ",
        "outputId": "8c32ea0b-6f51-452f-c74c-0fbf27385ed5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\r\n",
        "# conditions\r\n",
        "import lexnlp.extract.en.conditions\r\n",
        "_ = [print(condition) for condition in list(lexnlp.extract.en.conditions.get_conditions(text))]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('until', '4 Cases that cite this headnote  [2] Creditors’ Remedies Lien and Priority Under St.1821, prohibiting a levy on a crop', '')\n",
            "('until', 'on a growing crop, nor does such lien attach', '')\n",
            "('if', 'It was proved by the claimants, by the production of a written contract, that Harrison, on the twenty-second of May, 1840, in consideration that the claimants were involved, as indorsers for Burton & Harrison of Sumter county, and were then exposed to an execution, amounting to upwards of fourteen thousand dollars, bargained and sold to the claimants all his growing crop of cotton &c., consisting of one hundred and twenty acres, &c. Allen Harrison promised and obliged himself to give up his crop to the use of the claimants at any time to save them from suffering as his indorsers;', '')\n",
            "('when', 'The claimants came from Tennessee, (where they resided) about the first of September, 1840, bringing with them three or four white laborers, and took possession of the crop and slaves, and with the latter, and white laborers, gathered the cotton, prepared it for market, and', '')\n",
            "('if', 'The court charged the jury, that the plaintiff had no lien by virtue of his judgment, and execution on the growing crop; that Harrison had a right to convey it, without being in any manner restrained by them; that the writing adduced, was a sale of the crop, but', '')\n",
            "('when', 'it was not, and the lien of the fieri facias would have attached upon it,', '')\n",
            "('if', 'gathered, yet', '')\n",
            "('not subject to', 'the claimants obtained possession on the first of September, and controlled the gathering of the crop, then no lien attached, and it was', '')\n",
            "('until', 'Rep, 693;] and', '')\n",
            "('until', '167,] which declares it to be lawful to levy an execution on a planted crop,', '')\n",
            "('if', 'It is admitted that the contract between the defendant in execution, and the claimants, was in good faith,', '')\n",
            "('when', 'The defendant in execution might at any time have divested the interest which the contract vested in the claimants, by discharging their liability as his indorsers, or a judgment creditor might have satisfied the lien, and', '')\n",
            "('unless', 'We will then consider the writing under which the claimants assert a right, as a mortgage with a power to take possession any time during the year,', '')\n",
            "('if', 'Conceding the truth of the facts stated in the bill of exceptions, and we think it will not follow, that the possession of the claimants is a nullity, and that the case must be considered as', '')\n",
            "('if', 'The contract contains an express undertaking to give up the crop at any time the claimants might require it for their indemnity, and', '')\n",
            "('if', 'they took possession of it in the absence of the grantor, (though without his consent,)', '')\n",
            "('if', 'he subsequently acquiesced in it, the inference would be,', '')\n",
            "('subject to', 'Mr. Dane, in remarking upon this point, says, “The American editor of Bacon’s Abridgment, says, ‘Wheat growing in the ground is a chattel, and', '')\n",
            "('until', 'The first section of the act of 1821, “To prevent sheriffs and other officers from levying executions in certain cases, enacts, that “It shall not be lawful for any sheriff or other officer, to levy a writ of fieri facias or other execution on the planted crop of a debtor, or person against whom an execution may issue,', '')\n",
            "('until', 'Now here is an express inhibition to levy an execution on a crop while it remains on, or in the ground, and', '')\n",
            "('until', 'If so, the act cited, will only have the effect of keeping the right to levy it in abeyance', '')\n",
            "('if', 'The lien and the right to levy are intimately connected, and', '')\n",
            "('until', 'That it was competent for the legislature to have made it unlawful to levy an execution on particular property,', '')\n",
            "('until', 'If the object was merely to suspend the sale,', '')\n",
            "('as soon as', 'The idea that the lien attached upon the planted crop', '')\n",
            "('until', 'the execution was delivered to the sheriff, though the right to levy it was postponed', '')\n",
            "('if', 'They do not refer to the lien,', '')\n",
            "('until', 'they did they would postpone it', '')\n",
            "('until', 'the crop was gathered; but it is the levy they relate to and postpone', '')\n",
            "('until', '**4 The right to levy an execution on a planted crop, then, being expressly taken away by the statute, the lien which is connected with and consequent upon that right, never attaches', '')\n",
            "('if', 'The circuit judge may have mistaken the law in supposing that the contract was a sale, but', '')\n",
            "('when', 'There is no assumption of any material fact in the charge; but the possession of the claimant, the time', '')\n",
            "('if', 'acquired, the gathering of the crop, &c., are all referred to the determination of the jury; who are instructed,', '')\n",
            "('until', '**4 The statute which presents the question before the court is, that “it shall not be lawful for any sheriff or other officer to levy a writ of fieei facias or other execution, on the planted crop of a debtor, or person against whom an execution may issue,', '')\n",
            "('subject to', 'The policy of the State, as indicated by these statutes, is undeniably that all the property of a debtor, real and personal, to which he has a legal title, shall be', '')\n",
            "('until', 'The mischief which the statute designed to remedy was, the sacrifice which would be necessarily made by the sale of an immature crop: the statute enables the debtor to retain it', '')\n",
            "('if', '**5', '')\n",
            "('until', 'The sheriff is forbidden to levy on a “planted crop”', '')\n",
            "('if', 'Now,', '')\n",
            "('until', 'This, I feel a thorough conviction, was not the intention of the legislature; but that it was to secure him from loss, by prohibiting a levy and sale of the crop,', '')\n",
            "('when', 'it was gathered,', '')\n",
            "('subject to', 'Growing crops as', '')\n",
            "('subject to', '464 Generally, at common law, growing crops raised by annual planting, while still attached to the soil, are regarded as personal chattels,', '')\n",
            "('where', 'And', '')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N_O5vJAPaaj",
        "outputId": "84d33bc0-4813-4b09-a730-1c2c73aba5a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# constraints\r\n",
        "import lexnlp.extract.en.constraints\r\n",
        "_ = [print(constraint) for constraint in list(lexnlp.extract.en.constraints.get_constraints(text))]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('after', 'on a growing crop, nor does such lien attach until', '')\n",
            "('after', '', ' and that alias and pluries fieri facias’, issued regularly up to the time levy was made; that the cotton levied on was growed on the plantation of harrison, and cultivated by the hands in his service.')\n",
            "('first of', 'the claimants came from tennessee, (where they resided) about the', '')\n",
            "('first of', 'the court charged the jury, that the plaintiff had no lien by virtue of his judgment, and execution on the growing crop; that harrison had a right to convey it, without being in any manner restrained by them; that the writing adduced, was a sale of the crop, but if it was not, and the lien of the fieri facias would have attached upon it, when gathered, yet if the claimants obtained possession on the', '')\n",
            "('after', 'it merely inhibits the levy, but the lien attaches, and a levy and sale may be made', '')\n",
            "('more than', 'taking this to be clear *744 law, and it will be seen, that the defendant in execution at the time of the levy had nothing', '')\n",
            "('before', 'it has been frequently mooted whether, at common law, corn, &c.,', '')\n",
            "('before', '**4 the statute which presents the question', '')\n",
            "('after', 'now, if the view taken by the majority of the court, is correct, the right secured to the plaintiff in execution, of levying on the crop', '')\n",
            "('before', 'tried', '')\n",
            "('before', 'tried', '')\n",
            "('before', 'tried', '')\n",
            "('before', 'tried', '')\n",
            "('before', 'tried', '')\n",
            "('before', 'tried', '')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvwL2JWxPhsR",
        "outputId": "61e3373f-1e29-441a-e2e0-2d2154360f0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# copyrights\r\n",
        "import lexnlp.extract.en.copyright\r\n",
        "_ = [print(copyright) for copyright in list(lexnlp.extract.en.copyright.get_copyright(text))]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('©', '2019', 'Thomson Reuters. No')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_OpWkuPPmxu",
        "outputId": "311e31b7-397e-4a14-c39d-344beb7946fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "#courts\r\n",
        "import pandas\r\n",
        "import lexnlp.extract.en.courts\r\n",
        "import lexnlp.extract.en.dict_entities\r\n",
        "\r\n",
        "court_df = pandas.read_csv(\"https://raw.githubusercontent.com/LexPredict/lexpredict-legal-dictionary/1.0.5/en/legal/us_courts.csv\")\r\n",
        "# Create config objects\r\n",
        "court_config_data = [lexnlp.extract.en.dict_entities.entity_config(0, \"Eastern District of Virginia\", 0, [\"E.D. Va.\"]),\r\n",
        "    lexnlp.extract.en.dict_entities.entity_config(1, \"Western District of Virginia\", 0, [\"W.D. Va.\"])]\r\n",
        "for entity, alias in lexnlp.extract.en.courts.get_courts(text, court_config_data):\r\n",
        "  print(\"entity=\", entity)\r\n",
        "  print(\"alias=\", alias)\r\n",
        "entity= (98, 'Eastern District of Virginia', 0, [('Eastern District of Virginia', None, False, None), ('E.D. Va.', None, False, None)])\r\n",
        "alias= ('E.D. Va.', None, False, None)\r\n",
        "entity= (70, 'Southern District of New York', 0, [('Southern District of New York', None, False, None), ('S.D.N.Y.', None, False, None)])\r\n",
        "alias= ('S.D.N.Y.', None, False, None)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-34d3858e720f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcourt_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://raw.githubusercontent.com/LexPredict/lexpredict-legal-dictionary/1.0.5/en/legal/us_courts.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create config objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m court_config_data = [lexnlp.extract.en.dict_entities.entity_config(0, \"Eastern District of Virginia\", 0, [\"E.D. Va.\"]),\n\u001b[0m\u001b[1;32m      9\u001b[0m     lexnlp.extract.en.dict_entities.entity_config(1, \"Western District of Virginia\", 0, [\"W.D. Va.\"])]\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malias\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlexnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcourts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_courts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcourt_config_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'lexnlp.extract.en.dict_entities' has no attribute 'entity_config'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvzU8wYnPsWX",
        "outputId": "273978d8-0f56-445b-e264-b03ff50277e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# CUSIP\r\n",
        "import lexnlp.extract.en.cusip\r\n",
        "list(lexnlp.extract.en.cusip.get_cusip(text))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac4VKMmbPvh9",
        "outputId": "89978e2e-a7fa-413f-94f6-aa4220296e74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# dates\r\n",
        "import lexnlp.extract.en.dates\r\n",
        "_ = [print(date) for date in list(lexnlp.extract.en.dates.get_dates(text))]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-01\n",
            "1840-11-01\n",
            "1839-10-01\n",
            "1840-09-01\n",
            "1840-05-01\n",
            "1840-05-01\n",
            "2021-12-01\n",
            "2021-12-01\n",
            "2021-01-01\n",
            "2021-01-01\n",
            "2021-01-01\n",
            "2021-03-21\n",
            "2021-06-01\n",
            "2021-07-01\n",
            "2021-11-01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR9HBIoAPvXO",
        "outputId": "d0e3b60e-6039-42e1-b4f2-2bb4c0a6178c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# definitions\r\n",
        "import lexnlp.extract.en.definitions\r\n",
        "list(lexnlp.extract.en.definitions.get_definitions(text))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQbCv7VHPzCL",
        "outputId": "9844d7a1-8e54-4d5b-db0c-2764bf8f8204",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# distances\r\n",
        "import lexnlp.extract.en.distances\r\n",
        "list(lexnlp.extract.en.distances.get_distances(text))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoN_rYJqP6CC",
        "outputId": "4d0d9d65-f88f-4233-853a-2e348c3b5ae5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#durations\r\n",
        "import lexnlp.extract.en.durations\r\n",
        "list(lexnlp.extract.en.durations.get_durations(text))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('second', Decimal('20.0'), Decimal('0.0002')),\n",
              " ('year', Decimal('6.0'), Decimal('2190.0'))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt-_5F-kP-vv",
        "outputId": "7c7cf131-bc21-4c06-8b0c-789a874c7e74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "# geoentities\r\n",
        "import lexnlp.extract.en.geoentities\r\n",
        "lexnlp.extract.en.geoentities.get_geoentities(text)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-2aa3b22cb9d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# geoentities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlexnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoentities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlexnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoentities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_geoentities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: get_geoentities() missing 1 required positional argument: 'geo_config_list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMe3ofGhQBZm",
        "outputId": "f477c0a8-7899-43cc-9ce6-2938e5afb62c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#money\r\n",
        "import lexnlp.extract.en.money\r\n",
        "list(lexnlp.extract.en.money.get_money(text))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Decimal('100.0'), 'USD'),\n",
              " (Decimal('14000.0'), 'USD'),\n",
              " (Decimal('14000.0'), 'USD')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lubLDX9KQEe5",
        "outputId": "05628838-448a-4f17-a08b-155966560fa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#percents\r\n",
        "import lexnlp.extract.en.percents\r\n",
        "list(lexnlp.extract.en.percents.get_percents(text))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1MZQpfUQKoK",
        "outputId": "2acc480d-c7ef-4d38-9b86-6581f0c24b6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#pii \r\n",
        "import lexnlp.extract.en.pii\r\n",
        "list(lexnlp.extract.en.pii.get_pii(text))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F247EqjQNFq",
        "outputId": "09042497-2319-4d30-eb6c-c70562dfb94f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#ratios\r\n",
        "import lexnlp.extract.en.ratios\r\n",
        "list(lexnlp.extract.en.ratios.get_ratios(text))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDE1I02vQQPu",
        "outputId": "e66a2b0c-4330-4edc-841e-d60a5f1dda57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#regulations\r\n",
        "import lexnlp.extract.en.regulations\r\n",
        "list(lexnlp.extract.en.regulations.get_regulations(text))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y_iVQFXQTbP",
        "outputId": "198cb6e6-96f7-4866-87e1-6eb01b99916e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# trademarks\r\n",
        "import lexnlp.extract.en.trademarks\r\n",
        "list(lexnlp.extract.en.trademarks.get_trademarks(text))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2LuIsIFQWi6",
        "outputId": "9a12b522-376a-4df0-aec3-0c691146485c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#urls\r\n",
        "import lexnlp.extract.en.urls\r\n",
        "list(lexnlp.extract.en.urls.get_urls(text))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}